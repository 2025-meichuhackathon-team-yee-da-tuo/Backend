#!/usr/bin/env python3

import asyncio
import time
import statistics
from datetime import datetime
from core.cache import cache
from core.redis_client import redis_client
from core.db import get_database

async def real_database_query(query_type: str, params: dict = None):
    """çœŸå¯¦çš„è³‡æ–™åº«æŸ¥è©¢ï¼ŒåŒ…å«å¯¦éš›çš„ MongoDB æ“ä½œ"""
    from core.db import get_database
    
    db = await get_database()
    
    if query_type == "get-all-items":
        collections = await db.list_collection_names()
        item_collections = [col for col in collections if col not in ["Trade-History"] and not col.startswith("user_")]
        return {
            "query_type": query_type,
            "params": params,
            "data": item_collections,
            "total_items": len(item_collections),
            "timestamp": time.time()
        }
    
    elif query_type == "trade-history":
        user = params.get("user", "") if params else ""
        limit = params.get("limit", -1) if params else -1
        
        if user != "":
            trade_history_collection = db[user]
        else:
            trade_history_collection = db["Trade-History"]
        
        cursor = trade_history_collection.find().sort("timestamp", -1).limit(limit) if limit > 0 else trade_history_collection.find().sort("timestamp", -1)
        trades = []
        async for trade in cursor:
            trade["_id"] = str(trade["_id"])
            trades.append(trade)
        
        return {
            "query_type": query_type,
            "params": params,
            "data": trades,
            "count": len(trades),
            "timestamp": time.time()
        }
    
    elif query_type == "recent-items":
        user = params.get("user", "") if params else ""
        limit = params.get("limit", -1) if params else -1
        
        if not user:
            return {"error": "éœ€è¦æŒ‡å®šä½¿ç”¨è€…"}
        
        user_collection = db[user]
        cursor = user_collection.find().sort("timestamp", -1).limit(limit) if limit > 0 else user_collection.find().sort("timestamp", -1)
        recent_items = []
        recent_items_set = set()
        async for trade in cursor:
            if trade["item_a"] not in recent_items_set:
                recent_items.append({"item": trade["item_a"]})
                recent_items_set.add(trade["item_a"])
        
        return {
            "query_type": query_type,
            "params": params,
            "data": recent_items,
            "count": len(recent_items),
            "timestamp": time.time()
        }
    
    elif query_type == "most-freq-trade":
        target = params.get("target", "") if params else ""
        limit = params.get("limit", -1) if params else -1
        
        if target:
            target_collection = db[target]
            pipeline = [
                {"$group": {"_id": {"$cond": [ {"$eq": ["$item_a", target]}, "$item_b", "$item_a"]}, "count": {"$sum": 1}}},
                {"$match": {"_id": {"$ne": None}}},
                {"$sort": {"count": -1}},
            ]
            if limit > 0:
                pipeline.append({"$limit": limit})
            cursor = target_collection.aggregate(pipeline)
            aggregation_results = await cursor.to_list(length=None)
            trade_pairs = []
            for result in aggregation_results:
                trade_pair = {
                    "trade_to": result["_id"],
                    "count": result["count"]
                }
                trade_pairs.append(trade_pair)
            return {
                "query_type": query_type,
                "params": params,
                "data": trade_pairs,
                "count": len(trade_pairs),
                "timestamp": time.time()
            }
        else:
            trade_history_collection = db["Trade-History"]
            pipeline = [
                {"$group": {"_id": "$item_a", "count": {"$sum": 1}}},
                {"$match": {"_id": {"$ne": None}}},
                {"$sort": {"count": -1}},
            ]
            if limit > 0:
                pipeline.append({"$limit": limit})
            cursor = trade_history_collection.aggregate(pipeline)
            aggregation_results = await cursor.to_list(length=None)
            trade_pairs = []
            for result in aggregation_results:
                trade_pair = {
                    "item": result["_id"],
                    "count": result["count"]
                }
                trade_pairs.append(trade_pair)
            return {
                "query_type": query_type,
                "params": params,
                "data": trade_pairs,
                "count": len(trade_pairs),
                "timestamp": time.time()
            }
    
    elif query_type == "fuzzy-search":
        from api.fuzzy_search import _fetch_all_item_names, _fuzzy_search_core
        
        q = params.get("q", "") if params else ""
        top_k = params.get("top_k", 10) if params else 10
        min_score = params.get("min_score", 0.20) if params else 0.20
        
        items = await _fetch_all_item_names()
        result_list = _fuzzy_search_core(q, items, top_k, min_score)
        
        return {
            "query_type": query_type,
            "params": params,
            "data": result_list,
            "count": len(result_list),
            "timestamp": time.time()
        }
    
    else:
        return {
            "query_type": query_type,
            "params": params,
            "data": f"æœªçŸ¥çš„æŸ¥è©¢é¡å‹: {query_type}",
            "timestamp": time.time()
        }

@cache(ttl=300, key_prefix="perf_test")
async def cached_database_query(query_type: str, params: dict = None):
    """æœ‰å¿«å–çš„è³‡æ–™åº«æŸ¥è©¢"""
    return await real_database_query(query_type, params)

async def uncached_database_query(query_type: str, params: dict = None):
    """ç„¡å¿«å–çš„è³‡æ–™åº«æŸ¥è©¢"""
    return await real_database_query(query_type, params)

async def test_single_query_performance():
    print("ğŸ” å–®æ¬¡æŸ¥è©¢æ•ˆèƒ½æ¸¬è©¦")
    print("=" * 50)
    
    query_type = "get-all-items"
    params = {"limit": 100}
    
    print("ğŸ“ æ¸¬è©¦ç„¡å¿«å–æŸ¥è©¢...")
    start_time = time.time()
    result1 = await uncached_database_query(query_type, params)
    uncached_time = (time.time() - start_time) * 1000
    print(f"ç„¡å¿«å–æŸ¥è©¢æ™‚é–“: {uncached_time:.2f}ms")
    
    print("\nğŸ“ æ¸¬è©¦æœ‰å¿«å–æŸ¥è©¢ï¼ˆç¬¬ä¸€æ¬¡ï¼Œå¿«å–æœªå‘½ä¸­ï¼‰...")
    start_time = time.time()
    result2 = await cached_database_query(query_type, params)
    cached_first_time = (time.time() - start_time) * 1000
    print(f"æœ‰å¿«å–æŸ¥è©¢æ™‚é–“ï¼ˆç¬¬ä¸€æ¬¡ï¼‰: {cached_first_time:.2f}ms")
    
    print("\nğŸ“ æ¸¬è©¦æœ‰å¿«å–æŸ¥è©¢ï¼ˆç¬¬äºŒæ¬¡ï¼Œå¿«å–å‘½ä¸­ï¼‰...")
    start_time = time.time()
    result3 = await cached_database_query(query_type, params)
    cached_second_time = (time.time() - start_time) * 1000
    print(f"æœ‰å¿«å–æŸ¥è©¢æ™‚é–“ï¼ˆç¬¬äºŒæ¬¡ï¼‰: {cached_second_time:.2f}ms")
    
    # è¨ˆç®—æ•ˆèƒ½æå‡
    speedup = uncached_time / cached_second_time
    print(f"\nâš¡ æ•ˆèƒ½æå‡: {speedup:.1f} å€")
    print(f"æ™‚é–“ç¯€çœ: {uncached_time - cached_second_time:.2f}ms")
    
    return {
        "uncached": uncached_time,
        "cached_first": cached_first_time,
        "cached_second": cached_second_time,
        "speedup": speedup
    }

async def test_batch_query_performance():
    print("\nğŸ”„ æ‰¹é‡æŸ¥è©¢æ•ˆèƒ½æ¸¬è©¦")
    print("=" * 50)
    
    queries = [
        ("get-all-items", {"limit": 100}),
        ("trade-history", {"user": "test_user"}),
        ("recent-items", {"user": "test_user", "limit": 50}),
        ("most-freq-trade", {"target": "gold"}),
        ("fuzzy-search", {"q": "è—è‰²ç±ƒå­", "top_k": 10}),
    ]
    
    print("ğŸ“ æ¸¬è©¦ç„¡å¿«å–æ‰¹é‡æŸ¥è©¢...")
    start_time = time.time()
    uncached_results = []
    for query_type, params in queries:
        result = await uncached_database_query(query_type, params)
        uncached_results.append(result)
    uncached_batch_time = (time.time() - start_time) * 1000
    print(f"ç„¡å¿«å–æ‰¹é‡æŸ¥è©¢æ™‚é–“: {uncached_batch_time:.2f}ms")
    
    print("\nğŸ“ æ¸¬è©¦æœ‰å¿«å–æ‰¹é‡æŸ¥è©¢ï¼ˆç¬¬ä¸€æ¬¡ï¼Œå…¨éƒ¨å¿«å–æœªå‘½ä¸­ï¼‰...")
    start_time = time.time()
    cached_first_results = []
    for query_type, params in queries:
        result = await cached_database_query(query_type, params)
        cached_first_results.append(result)
    cached_first_batch_time = (time.time() - start_time) * 1000
    print(f"æœ‰å¿«å–æ‰¹é‡æŸ¥è©¢æ™‚é–“ï¼ˆç¬¬ä¸€æ¬¡ï¼‰: {cached_first_batch_time:.2f}ms")
    
    print("\nğŸ“ æ¸¬è©¦æœ‰å¿«å–æ‰¹é‡æŸ¥è©¢ï¼ˆç¬¬äºŒæ¬¡ï¼Œå…¨éƒ¨å¿«å–å‘½ä¸­ï¼‰...")
    start_time = time.time()
    cached_second_results = []
    for query_type, params in queries:
        result = await cached_database_query(query_type, params)
        cached_second_results.append(result)
    cached_second_batch_time = (time.time() - start_time) * 1000
    print(f"æœ‰å¿«å–æ‰¹é‡æŸ¥è©¢æ™‚é–“ï¼ˆç¬¬äºŒæ¬¡ï¼‰: {cached_second_batch_time:.2f}ms")
    
    # è¨ˆç®—æ•ˆèƒ½æå‡
    batch_speedup = uncached_batch_time / cached_second_batch_time
    print(f"\nâš¡ æ‰¹é‡æŸ¥è©¢æ•ˆèƒ½æå‡: {batch_speedup:.1f} å€")
    print(f"æ‰¹é‡æŸ¥è©¢æ™‚é–“ç¯€çœ: {uncached_batch_time - cached_second_batch_time:.2f}ms")
    
    return {
        "uncached_batch": uncached_batch_time,
        "cached_first_batch": cached_first_batch_time,
        "cached_second_batch": cached_second_batch_time,
        "batch_speedup": batch_speedup
    }

async def test_concurrent_performance():
    print("\nğŸš€ ä½µç™¼æŸ¥è©¢æ•ˆèƒ½æ¸¬è©¦")
    print("=" * 50)
    
    concurrent_queries = 10
    query_type = "get-all-items"
    params = {"limit": 100}
    
    print(f"ğŸ“ æ¸¬è©¦ç„¡å¿«å–ä½µç™¼æŸ¥è©¢ï¼ˆ{concurrent_queries} å€‹ï¼‰...")
    start_time = time.time()
    uncached_tasks = [uncached_database_query(query_type, params) for _ in range(concurrent_queries)]
    uncached_results = await asyncio.gather(*uncached_tasks)
    uncached_concurrent_time = (time.time() - start_time) * 1000
    print(f"ç„¡å¿«å–ä½µç™¼æŸ¥è©¢æ™‚é–“: {uncached_concurrent_time:.2f}ms")
    
    print(f"\nğŸ“ æ¸¬è©¦æœ‰å¿«å–ä½µç™¼æŸ¥è©¢ï¼ˆç¬¬ä¸€æ¬¡ï¼Œ{concurrent_queries} å€‹ï¼‰...")
    start_time = time.time()
    cached_first_tasks = [cached_database_query(query_type, params) for _ in range(concurrent_queries)]
    cached_first_results = await asyncio.gather(*cached_first_tasks)
    cached_first_concurrent_time = (time.time() - start_time) * 1000
    print(f"æœ‰å¿«å–ä½µç™¼æŸ¥è©¢æ™‚é–“ï¼ˆç¬¬ä¸€æ¬¡ï¼‰: {cached_first_concurrent_time:.2f}ms")
    
    print(f"\nğŸ“ æ¸¬è©¦æœ‰å¿«å–ä½µç™¼æŸ¥è©¢ï¼ˆç¬¬äºŒæ¬¡ï¼Œ{concurrent_queries} å€‹ï¼‰...")
    start_time = time.time()
    cached_second_tasks = [cached_database_query(query_type, params) for _ in range(concurrent_queries)]
    cached_second_results = await asyncio.gather(*cached_second_tasks)
    cached_second_concurrent_time = (time.time() - start_time) * 1000
    print(f"æœ‰å¿«å–ä½µç™¼æŸ¥è©¢æ™‚é–“ï¼ˆç¬¬äºŒæ¬¡ï¼‰: {cached_second_concurrent_time:.2f}ms")
    
    # è¨ˆç®—æ•ˆèƒ½æå‡
    concurrent_speedup = uncached_concurrent_time / cached_second_concurrent_time
    print(f"\nâš¡ ä½µç™¼æŸ¥è©¢æ•ˆèƒ½æå‡: {concurrent_speedup:.1f} å€")
    print(f"ä½µç™¼æŸ¥è©¢æ™‚é–“ç¯€çœ: {uncached_concurrent_time - cached_second_concurrent_time:.2f}ms")
    
    return {
        "uncached_concurrent": uncached_concurrent_time,
        "cached_first_concurrent": cached_first_concurrent_time,
        "cached_second_concurrent": cached_second_concurrent_time,
        "concurrent_speedup": concurrent_speedup
    }

async def test_real_api_performance():
    print("\nğŸŒ çœŸå¯¦ API æ•ˆèƒ½æ¸¬è©¦")
    print("=" * 50)
    
    import httpx
    
    api_endpoints = [
        "http://localhost:8000/api/trade/get-all-items",
        "http://localhost:8000/api/trade/collections",
        "http://localhost:8000/api/cache/info",
    ]
    
    async with httpx.AsyncClient() as client:
        print("ğŸ“ æ¸¬è©¦ç¬¬ä¸€æ¬¡ API å‘¼å«ï¼ˆå¿«å–æœªå‘½ä¸­ï¼‰...")
        first_call_times = []
        for endpoint in api_endpoints:
            start_time = time.time()
            try:
                response = await client.get(endpoint)
                end_time = time.time()
                call_time = (end_time - start_time) * 1000
                first_call_times.append(call_time)
                print(f"  {endpoint}: {call_time:.2f}ms")
            except Exception as e:
                print(f"  {endpoint}: é€£ç·šå¤±æ•— - {e}")
        
        print("\nğŸ“ æ¸¬è©¦ç¬¬äºŒæ¬¡ API å‘¼å«ï¼ˆå¿«å–å‘½ä¸­ï¼‰...")
        second_call_times = []
        for endpoint in api_endpoints:
            start_time = time.time()
            try:
                response = await client.get(endpoint)
                end_time = time.time()
                call_time = (end_time - start_time) * 1000
                second_call_times.append(call_time)
                print(f"  {endpoint}: {call_time:.2f}ms")
            except Exception as e:
                print(f"  {endpoint}: é€£ç·šå¤±æ•— - {e}")
        
        if first_call_times and second_call_times:
            avg_first = statistics.mean(first_call_times)
            avg_second = statistics.mean(second_call_times)
            api_speedup = avg_first / avg_second
            print(f"\nâš¡ API å¹³å‡æ•ˆèƒ½æå‡: {api_speedup:.1f} å€")
            print(f"API å¹³å‡æ™‚é–“ç¯€çœ: {avg_first - avg_second:.2f}ms")
            
            return {
                "first_call_avg": avg_first,
                "second_call_avg": avg_second,
                "api_speedup": api_speedup
            }
    
    return None

async def generate_performance_report(results):
    print("\nğŸ“‹ æ•ˆèƒ½æ¸¬è©¦å ±å‘Š")
    print("=" * 60)
    
    report = {
        "timestamp": datetime.now().isoformat(),
        "test_results": results
    }
    
    import json
    with open("cache_performance_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print("âœ… æ•ˆèƒ½æ¸¬è©¦å ±å‘Šå·²å„²å­˜åˆ° cache_performance_report.json")
    
    print("\nğŸ¯ æ•ˆèƒ½æ¸¬è©¦ç¸½çµ")
    print("-" * 40)
    if "single" in results:
        print(f"å–®æ¬¡æŸ¥è©¢æ•ˆèƒ½æå‡: {results['single']['speedup']:.1f} å€")
    if "batch" in results:
        print(f"æ‰¹é‡æŸ¥è©¢æ•ˆèƒ½æå‡: {results['batch']['batch_speedup']:.1f} å€")
    if "concurrent" in results:
        print(f"ä½µç™¼æŸ¥è©¢æ•ˆèƒ½æå‡: {results['concurrent']['concurrent_speedup']:.1f} å€")
    if "api" in results and results["api"]:
        print(f"API æŸ¥è©¢æ•ˆèƒ½æå‡: {results['api']['api_speedup']:.1f} å€")

async def main():
    print("ğŸš€ å¿«å–æ•ˆèƒ½å°æ¯”æ¸¬è©¦é–‹å§‹")
    print(f"æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    results = {}
    
    try:
        results["single"] = await test_single_query_performance()
        results["batch"] = await test_batch_query_performance()
        results["concurrent"] = await test_concurrent_performance()
        
        try:
            api_results = await test_real_api_performance()
            if api_results:
                results["api"] = api_results
        except Exception as e:
            print(f"âš ï¸ çœŸå¯¦ API æ¸¬è©¦è·³é: {e}")
        
        await generate_performance_report(results)
        
        print("\nğŸ‰ æ•ˆèƒ½æ¸¬è©¦å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    asyncio.run(main())
